{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summarization with langchain\n",
    "\n",
    "Summarization of long documents is a common LLM use case. The issue that most often arises, however, is that there is a token limit for the model. (Max context window length). With langchain this can be worked around by chunking and recursive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: chromadb in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: langchain in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (0.0.312)\n",
      "Requirement already satisfied: filelock in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pandas>=1.3 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (1.5.3)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (1.10.15)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (0.7.1)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (1.17.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: anyio<4.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (0.6.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (0.0.92)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from anyio<4.0->langchain) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from anyio<4.0->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: fsspec in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2024.3.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: coloredlogs in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.26.1)\n",
      "Requirement already satisfied: sympy in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from pandas>=1.3->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from pandas>=1.3->chromadb) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: certifi in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: colorama in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=7.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\1_studi independen ibm academy_advance ai\\integration langchain and watsonx ai\\langchain\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# First lets install dependencies (make sure already installed)\n",
    "!pip3 install transformers chromadb langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# First import the dependencies we need:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Get our API key, projectId and URL from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can take a [stuff](https://python.langchain.com/docs/modules/chains/document/stuff) or [map reduce](https://python.langchain.com/docs/modules/chains/document/map_reduce) approach to summarizing documents. We'll start with the simpler \"stuff\". Feel free to play around with changing the document URL and inference parameters to optimize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading web document...\n",
      "Done.\n",
      "Initializing flan-ul2-20B model...\n",
      "Running summarization task...\n",
      "\n",
      "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI—generative AI, in particular—has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years. AI’s value is not limited to advances in industry and consumer products alone. When implemented in a responsible way—where the technology is fully governed, privacy is protected and decision making is transparent and explainable—AI has the power to usher in a new era of government services. Such services can empower citizens and help restore trust in public entities by improving workforce efficiency and reducing operational costs in the public sector. On the backend, AI likewise has the potential to supercharge digital modernization in by, for example, automating the migration of legacy software to more flexible cloud-based applications\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize llm and document loader:\n",
    "print(\"Loading web document...\")\n",
    "# Try out some other documents as well\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\n",
    "doc = loader.load()\n",
    "print(\"Done.\")\n",
    "\n",
    "# You might need to tweak some of the runtime parameters to optimize the results.\n",
    "print(\"Initializing flan-ul2-20B model...\")\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.15,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 20,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 205\n",
    "}\n",
    "\n",
    "flan_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# Can use 'stuff' or 'map reduce'; \n",
    "chain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n",
    "\n",
    "print(\"Running summarization task...\\n\")\n",
    "\n",
    "res = chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine several of the features we've seen previously, including prompt templates and chains. In the following block we load the document into a template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chain...\n",
      "Stuff chain with documents...\n",
      "Running summarization on stuffed document chain...\n",
      "\n",
      "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI—generative AI, in particular—has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years. AI’s value is not limited to advances in industry and consumer products alone. When implemented in a responsible way—where the technology is fully governed, privacy is protected and decision making is transparent and explainable—AI has the power to usher in a new era of government services. Such services can empower citizens and help restore trust in public entities by improving workforce efficiency and reducing operational costs in the public sector. On the backend, AI likewise has the potential to supercharge digital modernization in by, for example, automating the migration of legacy software to more flexible cloud-based applications\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLM chain\n",
    "print(\"Initializing chain...\")\n",
    "llm_chain = LLMChain(llm=flan_model, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "print(\"Stuff chain with documents...\")\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "print(\"Running summarization on stuffed document chain...\\n\")\n",
    "res = stuff_chain.run(doc)\n",
    "\n",
    "print(res)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output above should be the same as the previous block if using the same inference parameters and document URL. Now we will use the same stuff chain method to see how it behaves with multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2nd article...\n",
      "Done.\n",
      "Running summarization on stuffed document chain.\n",
      "\n",
      "Few technologies have taken the world by storm the way artificial intelligence (AI) has over the past few years. AI and its many use cases have become a topic of public discussion no longer relegated to tech experts. AI—generative AI, in particular—has tremendous potential to transform society as we know it for good, boost productivity and unlock trillions in economic value in the coming years. AI’s value is not limited to advances in industry and consumer products alone. When implemented in a responsible way—where the technology is fully governed, privacy is protected and decision making is transparent and explainable—AI has the power to usher in a new era of government services. Such services can empower citizens and help restore trust in public entities by improving workforce efficiency and reducing operational costs in the public sector. On the backend, AI likewise has the potential to supercharge digital modernization in by, for example, automating the migration of legacy software to more flexible cloud-based applications\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Add a new article\n",
    "print(\"Loading 2nd article...\")\n",
    "loader_2 = WebBaseLoader('https://www.ibm.com/industries/federal/autonomous-ship')\n",
    "doc_2 = loader_2.load() # Returns list\n",
    "print(\"Done.\")\n",
    "\n",
    "# Combine docs\n",
    "docs = doc + doc_2\n",
    "\n",
    "print(\"Running summarization on stuffed document chain.\\n\")\n",
    "try:\n",
    "  res = stuff_chain.run(docs)\n",
    "  print(res)\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above code should result in an error to the effect of `input tokens (7038) plus prefix length (0) must be < 4096` meaning that we have exceeded the model's token input length. This brings us to the next topic, \"map reduce\" which helps us solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3rd document...\n",
      "Init map chain...\n",
      "Init reduce chain...\n",
      "Stuff documents using reduce chain...\n",
      "Init chunk splitter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce82259679142e89f0da7a6ad5752e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1_Studi Independen IBM Academy_Advance AI\\Integration Langchain and Watsonx AI\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9732320ad345e18819bf9457dd8ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9c6ec7570b411dae9e6a9173450e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35c603e96bb41a3ba3f9bfc2958238d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518d8a0c63724428bd6c30b615c11f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 chunks: \n",
      "Run map-reduce chain. This should take ~15-30 seconds...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e202da59fd48abac040a8cf9c39372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92887cc8dbb9451db432a5812ddf3b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f68b9fcdd64e79af60940342aa7e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed8d2d8f58b4083b0c662c8ecaf285e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b105d5bd54b347bebd276dfb747e6d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Elapsed time: 16.79 seconds.\n",
      "\n",
      "Results from each chunk: \n",
      "\n",
      "1. No, I don't think so. I'm not interested in this topic. I'm interested in other topics.\n",
      "\n",
      "2. Federal employees are going to see AI tools show up in cloud-based productivity suites sooner rather than later, but it's not clear yet how the trending tech will impact public-facing digital services.\n",
      "\n",
      "3. How government agencies come to use generative AI and other innovative technologies in their operations will largely depend upon how the regulatory scheme unfolds\n",
      "\n",
      "\n",
      "\n",
      "Final output:\n",
      "\n",
      "No, I'm not interested in this topic. I'm interested in other topics. AI tools are going to show up in cloud-based productivity suites sooner rather than later, but it's not clear yet how the trending tech will impact public-facing digital services.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "# Add a 3rd document\n",
    "print(\"Loading 3rd document...\")\n",
    "loader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\n",
    "doc_3 = loader_3.load()\n",
    "docs = docs + doc_3\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n",
    "# You might want to play around with different tokenizers and text splitters to see how the results change.\n",
    "print(\"Init chunk splitter...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\") # Hugging face tokenizer for flan-ul2\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Using {len(split_docs)} chunks: \")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(split_docs)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    print(\"Results from each chunk: \\n\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        print(f\"{idx + 1}. {step}\\n\")\n",
    "    \n",
    "    print(\"\\n\\nFinal output:\\n\")\n",
    "    print(output)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
