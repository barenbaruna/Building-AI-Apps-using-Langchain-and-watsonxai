{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032112c4",
   "metadata": {},
   "source": [
    "## 1. Programmatically using WatsonX.ai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4adcdb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing dependencies.\n"
     ]
    }
   ],
   "source": [
    "# First import the dependencies we need:\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "try:\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
    "    from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
    "\n",
    "    from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "    from ibm_watson_machine_learning.foundation_models import Model\n",
    "    from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "except ImportError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Done importing dependencies.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96339420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done getting env variables.\n"
     ]
    }
   ],
   "source": [
    "# Get our API key and URL from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "print(\"Done getting env variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing LLM.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WatsonX model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 25,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "print(\"Done initializing LLM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris\n",
      "The capital of Japan is Tokyo\n",
      "The capital of Australia is Canberra\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "\n",
    "try:\n",
    "  for country in countries:\n",
    "    question = f\"What is the capital of {country}\"\n",
    "    res = llm_model.generate_text(question)\n",
    "    print(f\"The capital of {country} is {res.capitalize()}\")\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e2e8f",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates & Chains\n",
    "\n",
    "In the previous example, the user input is sent directly to the Watsonx LLM, without using Langchain. This is a basic use case, but real applications are rarely so simple. When using an LLM in an application, you will usually need to reuse the same prompt across multiple scenarios. We will now replicate the previous example, but use an LLM chain. This allows us to:\n",
    "\n",
    "- Accept user input and contruct a prompt\n",
    "- Generate multiple prompts from a collection of data points in a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310c2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Sweden? = Stockholm\n",
      "What is the capital of Mexico? = Mexico city\n",
      "What is the capital of Vietnam? = Hanoi\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=[\"country\"],\n",
    "  template= \"What is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"Sweden\", \"Mexico\", \"Vietnam\"]\n",
    "  for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a9df3",
   "metadata": {},
   "source": [
    "## 3. Simple sequential chains\n",
    "The utility of LangChain becomes apparent as we chain outputs of one model as input to another model. Here's a simple example where one generates a question which the other model answers.\n",
    "\n",
    "LangChain determines a model's output based on its response.  In our examples, the first model creates a response to the end prompt of \"Question:\" which LangChain maps as an input variable called \"question\" which it passes to the 2nd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffda7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e4e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate 2 models (Note, these could be different models depending on use case)\n",
    "# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\n",
    "model_1 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "model_2 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35de1e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Construct the sequential chain\n",
    "prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n",
    "prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34586549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mWhat is the name of the largest lizard in the world?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mgila monster\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run our chain with the topic: \"an animal\"\n",
    "# Play around with providing different topics to see the output. eg. cars, the Roman empire\n",
    "try:\n",
    "  qa.run(\"an animal\")\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7c152d",
   "metadata": {},
   "source": [
    "## 4. Easy Loading of Documents Using Lang Chain\n",
    "LangChain makes it easy to extract passages from documents so that you can answer questions based on your document's content. First download the example PDF file to your working folder: [what-is-generative-ai.pdf](https://github.com/ibm-build-lab/VAD-VAR-Workshop/blob/main/content/Watsonx/WatsonxAI/105/what-is-generative-ai.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1743ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load PDF document\n",
    "pdf='what is generative ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9348f8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff570a5f8c6143b998a6e46043c86f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6360620232e8432fa7f527686445f854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3119901ab953408984db10db9fafa656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcae85334684c96816facc76e335575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625050b3505f4aafb46702b54d93d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdd0257d3b3433db53c9950a0891035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202a93c83c50484b8a68bad9d5a78725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cb1622cb334cecb20188b410b98e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282ebda3d3544ff0b71b7b766f345739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30300622856641fe902e16bc2943de10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15945c3514bc42e2ac421d6dd4be78cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab44452ea9343e2bc626a68cfbea9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45ed2d859304d10b82cadf7151cd5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55a26500ac6438686a3fbc21e872b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296a10cf12744b2e8a1e23e7e2e43aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\1_Studi Independen IBM Academy_Advance AI\\Integration Langchain and Watsonx AI\\langchain\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Index loaded PDF\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a890f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Initialize watsonx google/flan-ul2 model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300\n",
    "}\n",
    "model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d1a50d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Init RAG chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6efed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a type of artificial intelligence. Through machine learning, practitioners develop artificial intelligence through models that can “learn” from data patterns without human direction. The unmanageably huge volume and complexity of data (unmanageable by humans, anyway) that is now being generated has increased the potential of machine learning, as well as the need for it.\n"
     ]
    }
   ],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"what is Machine Learning?\")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2679f2",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) is a common AI use case. Many companies have vast amounts of data about which they want an AI system to answer questions, do searches or perform summarization tasks. We will learn more about RAG in lab 106."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
